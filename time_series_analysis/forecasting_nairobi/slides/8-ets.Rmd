---
title: "Exponential smoothing models"
author: ""
date: ""
toc: true
colortheme: monashwhite
output:
  binb::monash:
    fig_width: 7
    fig_height: 3.5
    includes:
      in_header: header.tex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache=TRUE, warning=FALSE, message=FALSE)

library(tidyverse)
library(fable)
library(tsibble)
library(feasts)
library(lubridate)
library(tsibbledata)

options(digits=4, width=55)
```

# Exponential Smoothing methods

## Average method
\fontsize{14}{16}\sf
  * Forecast of all future values is equal to mean of historical data $\{y_1,\dots,y_T\}$.
  * Forecasts: $\hat{y}_{T+h|T} = \bar{y} = (y_1+\dots+y_T)/T$

```{r mean-method-explained, echo=FALSE, message=FALSE, warning=FALSE, fig.height = 3.3}
bricks <- aus_production %>%
  filter(!is.na(Bricks)) %>%
  mutate(average = mean(Bricks))

fc <- bricks %>%
  filter(row_number() == n()) %>% as_tibble() %>% 
  unnest(Quarter = list(as.Date(Quarter) + months(c(0, 12*5))))

bricks %>%
  ggplot(aes(x = Quarter, y = Bricks)) +
  geom_line() +
  geom_line(aes(y = average), colour = "blue", linetype = "dashed") +
  geom_line(aes(y = average), data = fc, colour = "blue") +
  ggtitle("Clay brick production in Australia")
```

## 
\begin{block}{Average forecasts}
  \centerline{$\displaystyle\pred{y}{T+h}{T} = \frac1T\sum_{t=1}^T y_t$}
\end{block}\pause\vspace*{-0.2cm}

* Want something in between these methods.
* Most recent data should have more weight.
* Trend and seasonality

## Historical perspective

 * Developed in the 1950s and 1960s as **methods** (algorithms) to produce point forecasts.
 * Combine a "level", "trend" (slope) and "seasonal" component to describe a time series.
 * The rate of change of the components are controlled by "smoothing parameters":\newline $\alpha$, $\beta$ and $\gamma$ respectively.
  * Need to choose best values for the smoothing parameters (and initial states).
  * Equivalent ETS state space **models** developed in the 1990s and 2000s.

## Pegel's classification

```{r pegel, out.width='95%'}
knitr::include_graphics("figs/es5.PNG")
```

## Exponential smoothing methods
\fontsize{12}{14}\sf

\begin{block}{}
\begin{tabular}{ll|ccc}
& &\multicolumn{3}{c}{\bf Seasonal Component} \\
\multicolumn{2}{c|}{\bf Trend}& N & A & M\\
\multicolumn{2}{c|}{\bf Component}  & (None)    & (Additive)  & (Multiplicative)\\
\cline{3-5} &&&&\\[-0.4cm]
N & (None) & (N,N) & (N,A) & (N,M)\\
&&&&\\[-0.4cm]
A & (Additive) & (A,N) & (A,A) & (A,M)\\
&&&&\\[-0.4cm]
A\damped & (Additive damped) & (A\damped,N) & (A\damped,A) & (A\damped,M)
\end{tabular}
\end{block}\fontsize{12}{14}\sf

\begin{tabular}{lp{9.7cm}}
\textcolor[rgb]{0.90,0.,0.00}{(N,N)}:        &Simple exponential smoothing\\
\textcolor[rgb]{0.90,0.,0.00}{(A,N)}:        &Holt's linear method\\
\textcolor[rgb]{0.90,0.,0.00}{(A\damped,N)}: &Additive damped trend method\\
\textcolor[rgb]{0.90,0.,0.00}{(A,A)}:~~ &Additive Holt-Winters' method\\
\textcolor[rgb]{0.90,0.,0.00}{(A,M)}: &Multiplicative Holt-Winters' method\\
\textcolor[rgb]{0.90,0.,0.00}{(A\damped,M)}: &Damped multiplicative Holt-Winters' method
\end{tabular}

\begin{block}{}\fontsize{12}{14}\sf
There are also multiplicative trend methods (not recommended).
\end{block}

## Capturing levels, trends, and seasonalities

We want a method that captures the level ($\ell_t$), trend ($b_t$) and seasonality ($s_t$). 

How do we combine these elements?

\pause

### Additively?

$y_t = \ell_{t-1} + b_{t-1} + s_{t-m}$

\pause

### Multiplicatively?

$y_t = \ell_{t-1}b_{t-1}s_{t-m}$

\pause

### Perhaps a mix of both?

$y_t = (\ell_{t-1} + b_{t-1}) s_{t-m}$


## Exponential smoothing methods

```{r esm, fig.align='center', out.width="98%"}
knitr::include_graphics("figs/es4.PNG")
```

\pause

\fontsize{11}{13}\sf
\alert{How do the level, trend and seasonal components evolve over time?}

## Big idea: control the rate of change (smoothing)
\fontsize{14}{15}\sf

$\alpha$ controls the flexibility of the **level**

* If $\alpha = 0$, the level never updates (mean)
* If $\alpha = 1$, the level updates completely (naive)

$\beta$ controls the flexibility of the **trend**

* If $\beta = 0$, the trend is linear (regression trend)
* If $\beta = 1$, the trend updates every observation

$\gamma$ controls the flexibility of the **seasonality**

* If $\gamma = 0$, the seasonality is fixed (seasonal means)
* If $\gamma = 1$, the seasonality updates completely (seasonal naive)

## Parameter estimation

- Need to choose value for smoothing and initial values.
- A robust and objective way to obtain values for the unknown parameters included in any exponential smoothing method is to estimate them from the observed data.
- Initial and smoothing values are chosen by minimising sum of squared errors (SSE):
$${SSE}=\sum_{t=1}^N(y_t-{\hat y}_{t|t-1})^2=\sum_{t=1}^Ne_t^2.$$

# Exponential smoothing state space models

###  methods
- ES methods presented so far are algorithms that generate **point forecast**. 
- Each exponential smoothing method can be written as an **Innovation state space model**

### models
  * Generate same point forecasts but can also generate forecast intervals.
  * A stochastic (or random) data generating process that can generate an entire forecast distribution.
  * Allow for "proper" model selection. Parameters are estimated using miximizing **likelihood**, the probability of the data arising from the specific model, instead of minimizing the Sum of Squared Error.

## Exponential smoothing methods
\fontsize{12}{14}\sf

\begin{block}{}
\begin{tabular}{ll|ccc}
& &\multicolumn{3}{c}{\bf Seasonal Component} \\
\multicolumn{2}{c|}{\bf Trend}& N & A & M\\
\multicolumn{2}{c|}{\bf Component}  & (None)    & (Additive)  & (Multiplicative)\\
\cline{3-5} &&&&\\[-0.4cm]
N & (None) & (N,N) & (N,A) & (N,M)\\
&&&&\\[-0.4cm]
A & (Additive) & (A,N) & (A,A) & (A,M)\\
&&&&\\[-0.4cm]
A\damped & (Additive damped) & (A\damped,N) & (A\damped,A) & (A\damped,M)
\end{tabular}
\end{block}\fontsize{12}{14}\sf

\begin{tabular}{lp{9.7cm}}
\textcolor[rgb]{0.90,0.,0.00}{(N,N)}:        &Simple exponential smoothing\\
\textcolor[rgb]{0.90,0.,0.00}{(A,N)}:        &Holt's linear method\\
\textcolor[rgb]{0.90,0.,0.00}{(A\damped,N)}: &Additive damped trend method\\
\textcolor[rgb]{0.90,0.,0.00}{(A,A)}:~~ &Additive Holt-Winters' method\\
\textcolor[rgb]{0.90,0.,0.00}{(A,M)}: &Multiplicative Holt-Winters' method\\
\textcolor[rgb]{0.90,0.,0.00}{(A\damped,M)}: &Damped multiplicative Holt-Winters' method
\end{tabular}

\begin{block}{}\fontsize{12}{14}\sf
There are also multiplicative trend methods (not recommended).
\end{block}

## ETS models

- Two models for each method: one with additive and one with multiplicative errors (multiplicative error means the noise increases with level of series)
- The possibilities for each component are: 
    - Error ={A,M}
    - Trend ={N,A,A$_d$,M,M$_d$} 
    - Seasonal ={N,A,M}
  where N =none, A = additive, M = multiplicative, and $_d$=damped. 
- Each state space model can be labeled as ETS (Error, Trend, Seasonal). 

## ETS models

- Each model consists of an \textit{observation} equation that describes the observed data and \textit{transition}, one for each state (level, trend, seasonal), i.e., state space models, that describe how the unobserved components or states change over time. Hence these are referred to as *state space models*.

## ETS taxonomy

\begin{block}{}
\hspace*{-0.25cm}\begin{tabular}{l@{}p{2.3cm}@{}c@{}l}
\structure{General n\rlap{otation}}
    &       & ~E T S~  & ~:\hspace*{0.3cm}\textbf{E}xponen\textbf{T}ial \textbf{S}moothing               \\ [-0.2cm]
    & \hfill{$\nearrow$\hspace*{-0.1cm}}        & {$\uparrow$} & {\hspace*{-0.2cm}$\nwarrow$} \\
    & \hfill{\textbf{E}rror\hspace*{0.2cm}} & {\textbf{T}rend}      & {\hspace*{0.2cm}\textbf{S}eason}
\end{tabular}
\end{block}

\alert{\textbf{E}rror:} Additive (`"A"`) or multiplicative (`"M"`)
\pause

\alert{\textbf{T}rend:} None (`"N"`), additive (`"A"`), multiplicative (`"M"`), or damped (`"Ad"` or `"Md"`).
\pause

\alert{\textbf{S}easonality:} None (`"N"`), additive (`"A"`) or multiplicative (`"M"`)

## ETS taxonomy
\fontsize{11}{12}\sf

\begin{block}{}
\begin{tabular}{ll|ccc}
  \multicolumn{2}{l}{\alert{\bf Additive Error}} &        \multicolumn{3}{c}{\bf Seasonal Component}         \\
          \multicolumn{2}{c|}{\bf Trend}         &         N         &         A         &         M         \\
        \multicolumn{2}{c|}{\bf Component}       &     ~(None)~      &    (Additive)     & (Multiplicative)  \\ \cline{3-5}
           &                                     &                   &                   &  \\[-0.3cm]
  N        & (None)                              &       A,N,N       &       A,N,A       &    A,N,M     \\
           &                                     &                   &                   &  \\[-0.3cm]
  A        & (Additive)                          &       A,A,N       &       A,A,A       &    A,A,M     \\
           &                                     &                   &                   &  \\[-0.3cm]
  A\damped & (Additive damped)                   &   A,A\damped,N    &   A,A\damped,A    & A,A\damped,M
\end{tabular}
\end{block}

\begin{block}{}
\begin{tabular}{ll|ccc}
  \multicolumn{2}{l}{\alert{\bf Multiplicative Error}} &     \multicolumn{3}{c}{\bf Seasonal Component}      \\
             \multicolumn{2}{c|}{\bf Trend}            &      N       &         A         &        M         \\
           \multicolumn{2}{c|}{\bf Component}          &   ~(None)~   &    (Additive)     & (Multiplicative) \\ \cline{3-5}
           &                                           &              &                   &  \\[-0.3cm]
  N        & (None)                                    &    M,N,N     &       M,N,A       &      M,N,M       \\
           &                                           &              &                   &  \\[-0.3cm]
  A        & (Additive)                                &    M,A,N     &       M,A,A       &      M,A,M       \\
           &                                           &              &                   &  \\[-0.3cm]
  A\damped & (Additive damped)                         & M,A\damped,N &   M,A\damped,A    &   M,A\damped,M
\end{tabular}
\end{block}

## Additive error models

\placefig{0}{1.5}{width=12.8cm,trim=0 120 0 0,clip=true}{fig_7_ets_add.pdf}

## Multiplicative error models

\placefig{0}{1.5}{width=12.8cm,trim=0 120 0 0,clip=true}{fig_7_ets_multi.pdf}


## Innovations state space models
\fontsize{12}{14}\sf

Let $\bm{x}_t = (\ell_t, b_t, s_t, s_{t-1}, \dots, s_{t-m+1})$ and
$\varepsilon_t\stackrel{\mbox{\scriptsize iid}}{\sim}
\mbox{N}(0,\sigma^2)$.
\begin{block}{}
\begin{tabular}{lcl}
$y_t$ &=& $\underbrace{h(\bm{x}_{t-1})} +
\underbrace{k(\bm{x}_{t-1})\varepsilon_t}$\\
&& \hspace*{0.5cm}$\mu_t$ \hspace*{1.45cm} $e_t$ \\[0.2cm]
$\bm{x}_t$ &=& $f(\bm{x}_{t-1}) +
g(\bm{x}_{t-1})\varepsilon_t$\\
\end{tabular}
\end{block}

Additive errors
: \mbox{}\vspace*{-0.5cm}\newline
  $k(x)=1$.\qquad $y_t = \mu_{t} + \varepsilon_t$.

Multiplicative errors
: \mbox{}\vspace*{-0.5cm}\newline
  $k(\bm{x}_{t-1}) = \mu_{t}$.\qquad $y_t = \mu_{t}(1 + \varepsilon_t)$.\newline
  $\varepsilon_t = (y_t - \mu_t)/\mu_t$ is relative error.

## Innovations state space models

\structure{Estimation}\vspace*{0.5cm}

\begin{block}{}
\begin{align*}
L^*(\bm\theta,\bm{x}_0) &= n\log\!\bigg(\sum_{t=1}^n \varepsilon^2_t/k^2(\bm{x}_{t-1})\!\bigg) + 2\sum_{t=1}^n \log|k(\bm{x}_{t-1})|\\
&= -2\log(\text{Likelihood}) + \mbox{constant}
\end{align*}
\end{block}

## Estimating ETS models

  * Smoothing parameters $\alpha$, $\beta$, $\gamma$ and $\phi$, and the initial states $\ell_0$, $b_0$, $s_0,s_{-1},\dots,s_{-m+1}$ are estimated by maximising the "likelihood" = the probability of the data arising from the specified model.
  * For models with additive errors equivalent to minimising SSE.
  * For models with multiplicative errors, \textbf{not} equivalent to minimising SSE.

## Model selection
\fontsize{13}{15}\sf

\begin{block}{Akaike's Information Criterion}
\[
\text{AIC} = -2\log(\text{L}) + 2k
\]
\end{block}\vspace*{-0.2cm}
where $L$ is the likelihood and $k$ is the number of parameters initial states estimated in the model.\pause

\begin{block}{Corrected AIC}
\[
\text{AIC}_{\text{c}} = \text{AIC} + \frac{2(k+1)(k+2)}{T-k}
\]
\end{block}
which is the AIC corrected (for small sample bias).
\pause
\begin{block}{Bayesian Information Criterion}
\[
\text{BIC} = \text{AIC} + k(\log(T)-2).
\]
\end{block}

## ETS(A,N,N): Specifying the model

\fontsize{13}{15}\sf

```{r ann-spec, echo = TRUE, results = "hide"}
ETS(y ~ error("A") + trend("N") + season("N"))
```

# Models with no trend, no seasonality

## ETS(A,N,N): SES with additive errors

\begin{block}{}\vspace*{-0.4cm}
\begin{align*}
\text{Forecast equation}&& \hat{y}_{T+h|T} &= \ell_T \\
\text{Measurement equation}&& y_t &= \ell_{t-1} + \varepsilon_t\\
\text{State equation}&& \ell_t&=\ell_{t-1}+\alpha \varepsilon_t
\end{align*}
\end{block}
where $\varepsilon_t\sim\text{NID}(0,\sigma^2)$.
\pause

  * "innovations" or "single source of error" because equations have the same error process, $\varepsilon_t$.
  * Measurement equation: relationship between observations and states.
  * Transition/state equation(s): evolution of the state(s) through time.

\vspace*{10cm}

## ETS(M,N,N): SES with multiplicative errors

\begin{block}{}\vspace*{-0.4cm}
\begin{align*}
\text{Forecast equation}&& \hat{y}_{T+h|T} &= \ell_T \\
\text{Measurement equation}&& y_t &= \ell_{t-1}(1 + \varepsilon_t)\\
\text{State equation}&& \ell_t&=\ell_{t-1}(1+\alpha \varepsilon_t)
\end{align*}
\end{block}
where $\varepsilon_t\sim\text{NID}(0,\sigma^2)$.
\pause

  * Models with additive and multiplicative errors with the same parameters generate the same point forecasts but different prediction intervals.

# Models with trend

## Holt's linear trend

\begin{block}{Additive errors: ETS(A,A,N)}\vspace*{-0.4cm}
\begin{align*}
\text{Forecast equation}&& \hat{y}_{T+h|T} &= \ell_T + hb_T\\
\text{Measurement equation}&& y_t &= \ell_{t-1}+b_{t-1}+\varepsilon_t\\
\text{State equations}&&       \ell_t&=\ell_{t-1}+b_{t-1}+\alpha \varepsilon_t\\
&&      b_t&=b_{t-1}+\beta \varepsilon_t
\end{align*}
\end{block}
\pause

\begin{block}{Multiplicative errors: ETS(M,A,N)}\vspace*{-0.4cm}
\begin{align*}
\text{Forecast equation}&& \hat{y}_{T+h|T} &= \ell_T + hb_T\\
\text{Measurement equation}&& y_t &= (\ell_{t-1}+b_{t-1})(1+\varepsilon_t)\\
\text{State equations}&&       \ell_t&=(\ell_{t-1}+b_{t-1})(1+\alpha \varepsilon_t)\\
&&      b_t&=b_{t-1}+\beta \varepsilon_t
\end{align*}
\end{block}

## Damped trend additive

- Holt's linear model displays a constant trend (increasing or decreasing) indefinitely into the future. 
- The exponential trend model also includes exponential growth or decline.
- Empirical evidence indicates that these methods tend to over-forecast, especially for longer forecast horizons. 
- Motivated by this, a *dampening* parameter is introduced so that the trend approaches a flat line some time in the future.

## Damped trend additive

\begin{block}{Component form}\vspace*{-0.4cm}
\begin{align*}
\pred{y}{t+h}{t} &= \ell_{t} + (\phi+\phi^2 + \dots + \phi^{h})b_{t} \\
\ell_{t} &= \alpha y_{t} + (1 - \alpha)(\ell_{t-1} + \phi b_{t-1})\\
b_{t} &= \beta^*(\ell_{t} - \ell_{t-1}) + (1 -\beta^*)\phi b_{t-1}.
\end{align*}
\end{block}
\pause

  * Damping parameter $0<\phi<1$.
  * If $\phi=1$, identical to Holt's linear trend.
  * As $h\rightarrow\infty$, $\pred{y}{T+h}{T}\rightarrow \ell_T+\phi b_T/(1-\phi)$.
  * Short-run forecasts trended, long-run forecasts constant.

## ETS(A,Ad,N): Damped trend multiplicative
\fontsize{14}{16}\sf

\begin{block}{Additive errors}\vspace*{-0.2cm}
\begin{align*}
\text{Forecast equation}&& \hat{y}_{T+h|T} &= \ell_T +b_T^(\phi + \cdots + \phi^{h-1})\\
\text{Measurement equation}&& y_t &= (\ell_{t-1}+\phi b_{t-1})+(1+\varepsilon_t)\\
\text{State equations}&&       \ell_t&=(\ell_{t-1}+\phi b_{t-1})+(1+\alpha \varepsilon_t)\\
&&      b_t&=\phi b_{t-1}+\beta (\ell_{t-1}+\phi b_{t-1})\varepsilon_t
\end{align*}
\end{block}
\pause

<!-- ## Lab Session  -->

<!-- Try forecasting A&E attendance for the next h=42 days using an ETS model. -->
<!--  1. Use SES and Holt’s method to forecast “future” data. -->

# Models with seasonality

## Holt-Winters method

- Holt and Winters extended Holt's method to capture seasonality.
- There are two variations to this method that differ in the nature of the seasonal component: 
      - **Additive : ** The additive method is preferred when the seasonal variations are roughly constant through the series, 
      - **Multiplicative : ** the multiplicative method is preferred when the seasonal variations are changing proportional to the level of the series.

## \fontsize{16}{16}\sf\bfseries ETS(A,A,A): Holt-Winters additive 

\begin{block}{}\vspace*{-0.4cm}
\begin{align*}
\text{Forecast equation} && \hat{y}_{t+h|t} &= \ell_{t} + hb_{t} + s_{t+h-m(k+1)}\\
\text{Observation equation}&& y_t&=\ell_{t-1}+b_{t-1}+s_{t-m} + \varepsilon_t\\
\text{State equations}&& \ell_t&=\ell_{t-1}+b_{t-1}+\alpha \varepsilon_t\\
&&        b_t&=b_{t-1}+\beta \varepsilon_t \\
&&s_t &= s_{t-m} + \gamma\varepsilon_t
\end{align*}
\end{block}

* $k=$ integer part of $(h-1)/m$.
* $\sum_i s_i \approx 0$.
* Parameters:&nbsp; $0\le \alpha\le 1$,&nbsp; $0\le \beta^*\le 1$,&nbsp; $0\le \gamma\le 1-\alpha$&nbsp;  and $m=$  period of seasonality (e.g. $m=4$ for quarterly data).

## \fontsize{16}{16}\sf\bfseries ETS(M,A,M): Holt-Winters multiplicative 

\begin{block}{}\vspace*{-0.4cm}
\begin{align*}
\text{Forecast equation} && \hat{y}_{t+h|t} &= (\ell_{t} + hb_{t}) s_{t+h-m(k+1)}\\
\text{Observation equation}&& y_t&= (\ell_{t-1}+b_{t-1})s_{t-m}(1 + \varepsilon_t)\\
\text{State equations}&& \ell_t&=(\ell_{t-1}+b_{t-1})(1+\alpha \varepsilon_t)\\
&&        b_t&=b_{t-1}(1+\beta \varepsilon_t) \\
&&s_t &= s_{t-m}(1 + \gamma\varepsilon_t)
\end{align*}
\end{block}

* $k$ is integer part of $(h-1)/m$.
* $\sum_i s_i \approx m$.
* Parameters:&nbsp; $0\le \alpha\le 1$,&nbsp; $0\le \beta^*\le 1$,&nbsp; $0\le \gamma\le 1-\alpha$&nbsp;  and $m=$  period of seasonality (e.g. $m=4$ for quarterly data).

## Holt-Winters damped method

Often the single most accurate forecasting method for seasonal data:
\begin{block}{}\vspace*{-0.4cm}
\begin{align*}
\pred{y}{t+h}{t} &= [\ell_{t} + (\phi+\phi^2 + \dots + \phi^{h})b_{t}]s_{t+h-m(k+1)} \\
\ell_{t} &= \alpha(y_{t} / s_{t-m}) + (1 - \alpha)(\ell_{t-1} + \phi b_{t-1})\\
b_{t} &= \beta^*(\ell_{t} - \ell_{t-1}) + (1 - \beta^*)\phi b_{t-1}       \\
s_{t} &= \gamma \frac{y_{t}}{(\ell_{t-1} + \phi b_{t-1})} + (1 - \gamma)s_{t-m}
\end{align*}
\end{block}

# Lab Session 7

## Lab Session 7

Use ETS model to produce forecast for 42 days:

* Split daily time series into train and test
* Specify following models and train data:
    * single exponential smoothing  
    * holt-winter
    * automatic ETS()
* use glance, tidy and report functions to extract information from trained models
* Report forecast accuracy
* which model is more accurate?
